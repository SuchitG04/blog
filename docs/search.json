[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/tvdesktopclassifier/tv-and-desktop-monitor-classifier-chap1-blog.html",
    "href": "posts/tvdesktopclassifier/tv-and-desktop-monitor-classifier-chap1-blog.html",
    "title": "Train your first image classifier (AI) model!",
    "section": "",
    "text": "Want to get hands-on experience with AI? If so, this is the perfect tutorial for you! Pre-requisites for this tutorial:\n\nbeginner-level python\nbe able to use jupyter notebooks\na Kaggle account (go create one duh)\n\nThat‚Äôs all! If you don‚Äôt know how to use Jupyter notebooks, click here for a quick tutorial on both Jupyter notebooks and Kaggle.\nSo what exactly will we be creating in this tutorial? We are gonna train a Deep Learning model to identify if a given image is a CRT TV or a flat screen TV or a desktop monitor. We will be using a beginner friendly and widely used library called fastai. Sounds damn cool, at least to me :)\nNote 1: You need not break your head over what each line of code does. This tutorial is meant to give you impetus to delve into Deep Learning and a top level overview of it‚Äôs power :)\nNote 2: GPU needs to be enabled for this tutorial or you‚Äôll be spending hours training the model üòÇ. Specifically enable GPU P100. Also, to use GPUs you need to have your phone number verified, so go do that if you haven‚Äôt yet."
  },
  {
    "objectID": "posts/tvdesktopclassifier/tv-and-desktop-monitor-classifier-chap1-blog.html#introduction",
    "href": "posts/tvdesktopclassifier/tv-and-desktop-monitor-classifier-chap1-blog.html#introduction",
    "title": "Train your first image classifier (AI) model!",
    "section": "",
    "text": "Want to get hands-on experience with AI? If so, this is the perfect tutorial for you! Pre-requisites for this tutorial:\n\nbeginner-level python\nbe able to use jupyter notebooks\na Kaggle account (go create one duh)\n\nThat‚Äôs all! If you don‚Äôt know how to use Jupyter notebooks, click here for a quick tutorial on both Jupyter notebooks and Kaggle.\nSo what exactly will we be creating in this tutorial? We are gonna train a Deep Learning model to identify if a given image is a CRT TV or a flat screen TV or a desktop monitor. We will be using a beginner friendly and widely used library called fastai. Sounds damn cool, at least to me :)\nNote 1: You need not break your head over what each line of code does. This tutorial is meant to give you impetus to delve into Deep Learning and a top level overview of it‚Äôs power :)\nNote 2: GPU needs to be enabled for this tutorial or you‚Äôll be spending hours training the model üòÇ. Specifically enable GPU P100. Also, to use GPUs you need to have your phone number verified, so go do that if you haven‚Äôt yet."
  },
  {
    "objectID": "posts/tvdesktopclassifier/tv-and-desktop-monitor-classifier-chap1-blog.html#installing-modules",
    "href": "posts/tvdesktopclassifier/tv-and-desktop-monitor-classifier-chap1-blog.html#installing-modules",
    "title": "Train your first image classifier (AI) model!",
    "section": "2. Installing modules",
    "text": "2. Installing modules\nHere, we will be installing/updating the python modules necessary for this tutorial.\nRun the two following code blocks to do so.\n\nimport os\nfrom fastcore.all import *\nimport urllib.request\nfrom fastai.vision.all import *\nfrom fastdownload import download_url\n\niskaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')\n\nif iskaggle:\n    !pip install -Uqq fastai duckduckgo_search\n\nfrom duckduckgo_search import ddg_images\n\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npandas-profiling 3.6.2 requires requests&lt;2.29,&gt;=2.24.0, but you have requests 2.31.0 which is incompatible.\nlibrosa 0.10.0.post2 requires soundfile&gt;=0.12.1, but you have soundfile 0.11.0 which is incompatible.\napache-beam 2.44.0 requires dill&lt;0.3.2,&gt;=0.3.1.1, but you have dill 0.3.6 which is incompatible."
  },
  {
    "objectID": "posts/tvdesktopclassifier/tv-and-desktop-monitor-classifier-chap1-blog.html#glimpsing-our-data",
    "href": "posts/tvdesktopclassifier/tv-and-desktop-monitor-classifier-chap1-blog.html#glimpsing-our-data",
    "title": "Train your first image classifier (AI) model!",
    "section": "3. Glimpsing our data",
    "text": "3. Glimpsing our data\nWhat is AI without data? Data is arguably the most important thing in the field of AI/Data Science. AI models are trained using tons and tons of data. Let‚Äôs have a look at how our data looks like.\nIn the first block, we search and retrieve one URL for an image of a flat screen TV. We then download the image from the retrieved URL and open the image and repeat the process for a CRT TV in the subsequent code blocks.\n\ndef search_images(term, max_images=40):\n    print(f\"Searching for '{term}'\")\n    return L(ddg_images(term, max_results=max_images)).itemgot('image')\n\n\nurls = search_images('flat screen tv', max_images=1)\nurls[0]\n\nSearching for 'flat screen tv'\n\n\n/opt/conda/lib/python3.7/site-packages/duckduckgo_search/compat.py:60: UserWarning: ddg_images is deprecated. Use DDGS().images() generator\n  warnings.warn(\"ddg_images is deprecated. Use DDGS().images() generator\")\n/opt/conda/lib/python3.7/site-packages/duckduckgo_search/compat.py:64: UserWarning: parameter page is deprecated\n  warnings.warn(\"parameter page is deprecated\")\n/opt/conda/lib/python3.7/site-packages/duckduckgo_search/compat.py:66: UserWarning: parameter max_results is deprecated\n  warnings.warn(\"parameter max_results is deprecated\")\n\n\n'http://s4msungtelevision32.files.wordpress.com/2013/01/flat-screen-televisions.jpg'\n\n\n\ndest = 'flatscreentv.jpg'\n# Trying two libraries because they both are working erratically for me\n# try:\n#     download_url(urls[0], dest, show_progress=False)\n#     print(\"hi\")\n# except:\nurllib.request.urlretrieve(urls[0], dest)\n\n('flatscreentv.jpg', &lt;http.client.HTTPMessage at 0x7e8f7900d090&gt;)\n\n\n\nImage.open(dest).to_thumb(256, 256)\n\n\n\n\n\ntry:\n    download_url(search_images('crt tv', max_images=1)[0], 'crttv.jpg', show_progress=False)\nexcept:\n    urllib.request.urlretrieve(search_images('crt tv', max_images=1)[0], 'crttv.jpg')\n\nImage.open('crttv.jpg').to_thumb(256, 256)\n\nSearching for 'crt tv'"
  },
  {
    "objectID": "posts/tvdesktopclassifier/tv-and-desktop-monitor-classifier-chap1-blog.html#downloading-images",
    "href": "posts/tvdesktopclassifier/tv-and-desktop-monitor-classifier-chap1-blog.html#downloading-images",
    "title": "Train your first image classifier (AI) model!",
    "section": "4. Downloading images",
    "text": "4. Downloading images\nLet us now download a bunch of images to train our model and make them ready to be ‚Äúfed‚Äù into the model.\n\n# Downloading images into their respective directories\nsearches = 'flat screen tv', 'crt tv', 'desktop monitor'\npath = Path('tv_or_desktop')\nfrom time import sleep\n\nfor o in searches:\n    dest = (path/o)\n    dest.mkdir(exist_ok=True, parents=True)\n    download_images(dest, urls=search_images(o))\n    sleep(10)\n    resize_images(path/o, max_size=400, dest=path/o)\n\nSearching for 'flat screen tv'\nSearching for 'crt tv'\nSearching for 'desktop monitor'\n\n\n\n# Check and remove unopenable images\nfailed = verify_images(get_image_files(path))\nfailed.map(Path.unlink)\nlen(failed)\n\n7\n\n\n\n# Loading the data\ndls = DataBlock(\n    blocks=(ImageBlock, CategoryBlock),\n    get_items=get_image_files,\n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=parent_label,\n    item_tfms=[Resize(192, method='squish')]\n).dataloaders(path, bs=32)\n\ndls.show_batch(max_n=9)"
  },
  {
    "objectID": "posts/tvdesktopclassifier/tv-and-desktop-monitor-classifier-chap1-blog.html#training-our-model",
    "href": "posts/tvdesktopclassifier/tv-and-desktop-monitor-classifier-chap1-blog.html#training-our-model",
    "title": "Train your first image classifier (AI) model!",
    "section": "5. Training our model!",
    "text": "5. Training our model!\nThis is it people. ‚Äôtis time to train our model! For this example we use a model called ResNet18 with 18 layers. ResNet18 is pre-trained on ImageNet dataset. Therefore, we need not train it again, rather we fine tune it to recognize images from our dataset, i.e., flatscreen TV, CRT TV and desktop monitors.\n\nlearn = vision_learner(dls, resnet18, metrics=error_rate)\nlearn.fine_tune(5)\n\n/opt/conda/lib/python3.7/site-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and may be removed in the future, \"\n/opt/conda/lib/python3.7/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.542415\n1.480012\n0.523810\n00:06\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.913363\n1.170745\n0.476190\n00:01\n\n\n1\n0.876068\n0.878507\n0.333333\n00:01\n\n\n2\n0.637958\n0.758396\n0.238095\n00:01\n\n\n3\n0.499402\n0.701759\n0.190476\n00:01\n\n\n4\n0.409540\n0.639917\n0.190476\n00:01"
  },
  {
    "objectID": "posts/tvdesktopclassifier/tv-and-desktop-monitor-classifier-chap1-blog.html#classifying-images",
    "href": "posts/tvdesktopclassifier/tv-and-desktop-monitor-classifier-chap1-blog.html#classifying-images",
    "title": "Train your first image classifier (AI) model!",
    "section": "6. Classifying images",
    "text": "6. Classifying images\nCongratulations! You have trained your first (ig ü§∑) image classification model! Let us now put it to test and try classifying some images.\n\n# Predicting an image we had downloaded earlier\nlearn.predict(PILImage.create('crttv.jpg'))\n\n\n\n\n\n\n\n\n('crt tv', tensor(0), tensor([9.9726e-01, 5.5894e-04, 2.1807e-03]))\n\n\n\n# Predicting an image we had downloaded earlier\nlearn.predict(PILImage.create('flatscreentv.jpg'))\n\n\n\n\n\n\n\n\n('desktop monitor', tensor(1), tensor([6.4813e-04, 9.9257e-01, 6.7823e-03]))\n\n\n\n# download_url(search_images('desktop monitor', max_images=1)[0], dest='desktopmonitor.jpg', show_progress=False)\ntry:\n    download_url(search_images('desktop monitor', max_images=1)[0], 'desktopmonitor.jpg', show_progress=False)\nexcept:\n    urllib.request.urlretrieve(search_images('desktop monitor', max_images=1)[0], 'desktopmonitor.jpg')\n\nImage.open('desktopmonitor.jpg').to_thumb(256, 256)\n\nSearching for 'desktop monitor'\n\n\n\n\n\n\nlearn.predict(PILImage.create('desktopmonitor.jpg'))\n\n\n\n\n\n\n\n\n('desktop monitor', tensor(1), tensor([5.4398e-05, 9.9916e-01, 7.8518e-04]))\n\n\nWell, that‚Äôs not bad for our first model. It has a decent accuracy. For me, it got 2/3 predictions right.\nIf you have any questions don‚Äôt hesitate to message me on discord. And lastly, here‚Äôs the link to my notebook if you wannaplay around with it (click on the ‚Äúcopy and edit button‚Äù).\nThank you for reading my blog. You can reach out to me through my socials here:\n\nDiscord - LostSquid&gt;.&lt;#6436\nLinkedIn - /in/suchitg04/\n\nI hope to see you soon. Until then üëã"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AsquirousSpeaks",
    "section": "",
    "text": "Data Cleaning and Augmentation\n\n\nThe counter-intuitive idea of data cleaning after training a model and creating data by ourselves.\n\n\n\n\n\n\nJun 26, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrain your first image classifier (AI) model!\n\n\nGet a taste of AI/deep learning with this simple tutorial that trains an image classifier model!\n\n\n\n\n\n\nMay 29, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nNew to CS/tech? Here‚Äôs my journey so far\n\n\nIt‚Äôs been more than two years since the first time I got lustfully ü§§ attracted to CS‚Ä¶\n\n\n\n\n\n\nMay 18, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHey yo! Welcome to my blog peeps!\n\n\nHiya! Know more about my blog!\n\n\n\n\n\n\nMay 15, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Hey yo! Welcome to my blog peeps!",
    "section": "",
    "text": "Hey there! My name is Suchit. I am a freshman studying Information Science (or CS, IS is just a marketing term here XD) at RIT, Bangalore.\nIn this blog I hope to share my learnings and mostly tech stuff. My aim is to help the Suchit from a week or a month or an year ago, so that people going through the same problem/thoughts can gain better clarity. The added bonus is that I‚Äôll still be able to remember how I felt during that situation, therefore enabling me to provide you with help that‚Äôll actually help you.\nWith that said, you can expect my blogs to be short and have hints of GenZ humour here and there.\nFeel free to reach out to me through my socials:\n\nDiscord - LostSquid&gt;.&lt;#6436\n\nLinkedIn - /in/suchitg04/\n\nI hope to see you soon! Until then üëã"
  },
  {
    "objectID": "posts/my-journey/index.html",
    "href": "posts/my-journey/index.html",
    "title": "New to CS/tech? Here‚Äôs my journey so far",
    "section": "",
    "text": "We all wanna git gud at this whole programming thingy, have conversations related to that new technology and maybe participate in a hackathon some time soon, don‚Äôt we?\n\n\n\nPhoto by Chuck Fortner on Unsplash\n\n\nI know how it feels to be among so many people talking tech and you not even having a remote idea of what they are talking about. Even after learning programming and tinkering with tech for more than two years, I still feel the same when I come across some new tool or technology.\n\nWannabe physicist\nHow did I start, you ask? Well, I did not really ‚Äústart‚Äù. It just happened. In fact, I wanted to major in Physics and become an astrophysicist. Me and one of my best friends were like science duos making projects learning about quantum particles and the whole thing. But then boom, I come across a single board computer called Raspberry Pi. It‚Äôs a mini-computer sort of a thing, if you are curious.\nOh wait, but it doesn‚Äôt run your normal Windows, it uses Linux as its OS and it turns out that you can make stuff like a VPN server, AdBlocker, security camera and more such cool projects! But then you‚Äôll have to know about getting around Linux and learning terminal commands. That‚Äôs what I did!\n\n\nWannabe C whiz\nI then got another board (Arduino UNO) for no reason. Oh, what is this? I need to be a C pro to use this, is what my noob self thought. I then bought a C course from Udemy (which I didn‚Äôt even get halfway through btw). That course just wasn‚Äôt interesting for me.\n\n\nWannabe Linux whiz\nüëÄ I saw some cool (internet) people talking about using Linux on their computer! They were using it alongside Windows by dual booting, it seems. Now, I leave all my exam preparations and get on installing Linux on my laptop. And guess which distro I chose to get started with?\nArch Linux. Yes, out of all the distros out there I chose Arch (the path to masochism). Considered by many to be the toughest Linux distro to get started with. Humblebragging? Yes, a little.\nLegend has it that Suchit is still trying to customize his OS to the core and taking his masochism to the next level.\n\n\nWhat am I doing now?\nI‚Äôm glad you asked. I‚Äôm hella confused. I‚Äôm confused if I‚Äôm juggling too many balls with my two hands.\nI‚Äôm learning deep learning alongside web development and DSA. I also felt it is the right time to start blogging because bRo YoU dOn‚ÄôT hAvE mUcH tO dO, right? ‚Ä¶ right? üôÇ\nBut then I find solace in the fact that not everyone has it all figured out (at least that‚Äôs what they say).\n\nI‚Äôm still in the process of figuring out and organizing my shit. Join me in this journey, so that we can figure it out together.\nThank you for reading my blog. You can reach out to me through my socials here:\n\nDiscord - LostSquid&gt;.&lt;#6436\nLinkedIn - /in/suchitg04/\n\nI hope to see you soon. Until then üëã"
  },
  {
    "objectID": "posts/data-cleaning/index.html",
    "href": "posts/data-cleaning/index.html",
    "title": "Data Cleaning and Augmentation",
    "section": "",
    "text": "It is a pretty intuitive thought that data is cleaned before training a model so that the model achieves good accuracy. What‚Äôs data cleaning in the first place? Naively, it is removing unrelated data that might have creeped through or removing data unrelated to what the model needs in training. I believe you get my point.\nLet‚Äôs take an example of a model that you are building to, say, classify between the faces of Elon Musk and Mark Zuckerberg. You‚Äôll probably download the images for your dataset from Google or Bing or any such search engines. With Mark rumored to be an alien and people taking a huge liking to the ‚Äúfemale version‚Äù of Elon, it is very certain that your dataset will contain a few such memes.\n\n\nAlien Zuckerberg\n\n\n\nElona Musk\n\nNow, sifting through the data in the hopes of finding such memes can be tedious. How about we let the model decide what images are faulty? Here‚Äôs how it works. You quickly train a model, the ‚Äúlosses‚Äù and accuracies get recorded, and then you pop up some images in decreasing order of ‚Äúloss‚Äù and/or accuracy (there are some tools already that can do that). There you go, the model now helped you find some black sheeps in your data that the it found difficult to classify and/or has low confidence about."
  },
  {
    "objectID": "posts/data-cleaning/index.html#data-cleaning",
    "href": "posts/data-cleaning/index.html#data-cleaning",
    "title": "Data Cleaning and Augmentation",
    "section": "",
    "text": "It is a pretty intuitive thought that data is cleaned before training a model so that the model achieves good accuracy. What‚Äôs data cleaning in the first place? Naively, it is removing unrelated data that might have creeped through or removing data unrelated to what the model needs in training. I believe you get my point.\nLet‚Äôs take an example of a model that you are building to, say, classify between the faces of Elon Musk and Mark Zuckerberg. You‚Äôll probably download the images for your dataset from Google or Bing or any such search engines. With Mark rumored to be an alien and people taking a huge liking to the ‚Äúfemale version‚Äù of Elon, it is very certain that your dataset will contain a few such memes.\n\n\nAlien Zuckerberg\n\n\n\nElona Musk\n\nNow, sifting through the data in the hopes of finding such memes can be tedious. How about we let the model decide what images are faulty? Here‚Äôs how it works. You quickly train a model, the ‚Äúlosses‚Äù and accuracies get recorded, and then you pop up some images in decreasing order of ‚Äúloss‚Äù and/or accuracy (there are some tools already that can do that). There you go, the model now helped you find some black sheeps in your data that the it found difficult to classify and/or has low confidence about."
  },
  {
    "objectID": "posts/data-cleaning/index.html#data-augmentation",
    "href": "posts/data-cleaning/index.html#data-augmentation",
    "title": "Data Cleaning and Augmentation",
    "section": "Data Augmentation",
    "text": "Data Augmentation\nThe world‚Äôs gone dystopian and governments are crumbling, and Elon and Mark have decided to collaborate and take advantage of this calamity. You are the hero in this situation.\nYou observe that there is a lot of movement in and out of the abandoned Twitter office (Elon‚Äôs selling tickets to Mars to the elites lol). Looks like someone‚Äôs having some in-person meetings ü§®. Now you install a camera along with a image/video recognition model to alert you whenever Elon or Mark comes and goes in/out of the office, but you don‚Äôt get a single trigger for days!\nYou scratch your head thinking about what could be wrong for hours until you realise that you had trained the model using just headshots and few such ‚Äúpresentable‚Äù images that you scavenged from what‚Äôs left of the internet, but you have installed your camera in such a place that it does not get such good images.\nWhat‚Äôs the solution you may ask? This is where Data Augmentation comes into the picture. You use this technique and apply certain effects on the images like cropping of random parts of the image, applying different colour filters, distorting the image, etc. Not only does this expand the dataset, but it also enables the model to better understand the object it is learning (Elon and Mark, in this case).\nHere‚Äôs an example of what data augmentation does: \nData augmentation is particularly useful when you have a small dataset. It helps bring some variance and helps avoid overfitting if done properly. Give this interesting article a read: Regularization Effect of Data Augmentation.\nCover photo by Elƒ´na ArƒÅja.\nThank you for reading my blog. You can reach out to me through my socials here:\n\nDiscord - ‚Äúlostsquid.‚Äù\nLinkedIn - /in/suchitg04/\n\nI hope to see you soon. Until then üëã"
  }
]