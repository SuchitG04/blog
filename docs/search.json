[
  {
    "objectID": "posts/mnist/index.html",
    "href": "posts/mnist/index.html",
    "title": "Classifying handwritten digits (THE MNIST!)",
    "section": "",
    "text": "From MNIST being the first Machine Learning dataset I heard about to building a model for it, I can surely say Machine Learning is a fascinating subject to study! Building a “model” is more than just getting a model and training it. It involves:\n\nGetting the data\nPreprocessing the data, i.e., cleaning it, converting it into a format that the model can understand, etc.\nCreating the training, validation, and test (test data is not considered in this case) split\nTraining the model\nFine-tuning hyper-parameters based on the inferences gotten from the accuracy and other metrics over the validation dataset, and then improving the model (not done in this case)\n\nGiven above is the rough process of building a Machine Learning project. This project maintains a medium level of abstraction and doesn’t entirely utilize high-level functions but doesn’t go deep into the low-level implementations either. I aim to maintain an understandable and yet not-so-abstracted level of coding throughout. Let’s get started!\n\n\n\n\n\n\nImportant\n\n\n\nIf you are referring to this post as a guide, then you are expected to have the following pre-requisites to fully and deeply understand what’s going on:\n\nBasics of Python\nImporting modules, methods and calling them in Python\nA fundamental idea of how a typical Linear Regression model works (knowledge of simple neural networks recommended but not necessary)\nWillingness and the ability to google and read through documentation ;)\n\n\n\n\n\n\n\n\n\nSome useful resources\n\n\n\n\nHow does a neural net actually work?\nfastai MNIST chapter\nMy MNIST (2 digits) notebook (linked again later below)\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nIf I have trouble understanding a piece of code written by others, here’s what I do:\n\nTry to speak out loudly and explain the code to myself (rubber ducking).\nSearch the official docuementation or stack overflow and understand through examples.\nIf the above two approaches don’t work, then ask ChatGPT to explain the code to me. This step works no matter what!\n\n\n\n\n# Importing the necessary libraries and modules\n\nfrom fastai.vision.all import *\nfrom torchvision import datasets\nimport torchvision.transforms as transforms\nfrom torch.utils.data import SubsetRandomSampler, random_split\nfrom sklearn.model_selection import train_test_split\n\nmatplotlib.rc('image', cmap='Greys')"
  },
  {
    "objectID": "posts/mnist/index.html#introduction",
    "href": "posts/mnist/index.html#introduction",
    "title": "Classifying handwritten digits (THE MNIST!)",
    "section": "",
    "text": "From MNIST being the first Machine Learning dataset I heard about to building a model for it, I can surely say Machine Learning is a fascinating subject to study! Building a “model” is more than just getting a model and training it. It involves:\n\nGetting the data\nPreprocessing the data, i.e., cleaning it, converting it into a format that the model can understand, etc.\nCreating the training, validation, and test (test data is not considered in this case) split\nTraining the model\nFine-tuning hyper-parameters based on the inferences gotten from the accuracy and other metrics over the validation dataset, and then improving the model (not done in this case)\n\nGiven above is the rough process of building a Machine Learning project. This project maintains a medium level of abstraction and doesn’t entirely utilize high-level functions but doesn’t go deep into the low-level implementations either. I aim to maintain an understandable and yet not-so-abstracted level of coding throughout. Let’s get started!\n\n\n\n\n\n\nImportant\n\n\n\nIf you are referring to this post as a guide, then you are expected to have the following pre-requisites to fully and deeply understand what’s going on:\n\nBasics of Python\nImporting modules, methods and calling them in Python\nA fundamental idea of how a typical Linear Regression model works (knowledge of simple neural networks recommended but not necessary)\nWillingness and the ability to google and read through documentation ;)\n\n\n\n\n\n\n\n\n\nSome useful resources\n\n\n\n\nHow does a neural net actually work?\nfastai MNIST chapter\nMy MNIST (2 digits) notebook (linked again later below)\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nIf I have trouble understanding a piece of code written by others, here’s what I do:\n\nTry to speak out loudly and explain the code to myself (rubber ducking).\nSearch the official docuementation or stack overflow and understand through examples.\nIf the above two approaches don’t work, then ask ChatGPT to explain the code to me. This step works no matter what!\n\n\n\n\n# Importing the necessary libraries and modules\n\nfrom fastai.vision.all import *\nfrom torchvision import datasets\nimport torchvision.transforms as transforms\nfrom torch.utils.data import SubsetRandomSampler, random_split\nfrom sklearn.model_selection import train_test_split\n\nmatplotlib.rc('image', cmap='Greys')"
  },
  {
    "objectID": "posts/mnist/index.html#downloading-the-dataset",
    "href": "posts/mnist/index.html#downloading-the-dataset",
    "title": "Classifying handwritten digits (THE MNIST!)",
    "section": "1. Downloading the Dataset",
    "text": "1. Downloading the Dataset\nPyTorch’s torchvision has a module named datasets that has some some popular datasets available to download and store it in a directory specified by you. If the dataset already exists, then you just need to give it the path to the dataset and it’ll skip downloading it. Click here to know more.\n\ndset = datasets.MNIST(\n    root='/home/suchitg/mnist/dset/', # Creates a folder named 'dset' if it's not already created\n    train=True, # Specifies what set of data to download (train or test)\n    transform=transforms.Compose( # Applies image transformations\n        [transforms.ToTensor(),\n         transforms.RandomRotation(degrees=30)\n        ]\n    ),\n    download=True\n)\n\nprint(dset)\nprint(dset.data.size())\n\nDataset MNIST\n    Number of datapoints: 60000\n    Root location: /home/suchitg/mnist/dset/\n    Split: Train\n    StandardTransform\nTransform: Compose(\n               ToTensor()\n               RandomRotation(degrees=[-30.0, 30.0], interpolation=nearest, expand=False, fill=0)\n           )\ntorch.Size([60000, 28, 28])\n\n\ntorchvision.transforms.RandomRotation is used to augment the image being loaded into a dataloader by randomly rotating the images about 30°. The reason for selecting this transform was because different handwritings write different digits at varying angles. So, the transformation accomodates for this nuance.\n\ntype(dset)\n\ntorchvision.datasets.mnist.MNIST"
  },
  {
    "objectID": "posts/mnist/index.html#train-val-split-and-normalization",
    "href": "posts/mnist/index.html#train-val-split-and-normalization",
    "title": "Classifying handwritten digits (THE MNIST!)",
    "section": "2. Train-Val split and Normalization",
    "text": "2. Train-Val split and Normalization\nHere, we use sklearn.model_selection.train_test_split to split the data into training and validation datasets. But what is a validation dataset? When we train the model, we improve it by looking at the loss calculated on training set itself. But the accuracy (in this case, how many digits the model gets right) is calculated by inputting the images from the validation dataset into the model. That way, we can know how well the model performs on images that it hasn’t seen before. If the validation is significantly higher than the training loss, then that indicates a case of overfitting.\nIt is also worth noting that, here, we create a stratified train-val split. What that means is that the number of images from each class is roughly equal. This ensures that the model will be equally good in classifying all digits.\n\ntrain_x, valid_x, train_y, valid_y = train_test_split(dset.data, dset.targets, test_size=0.2, stratify=dset.targets)\n\nLet us now see an example of how a digit looks in our dataset.\n\nplt.imshow(train_x.numpy()[99]), train_y[99]\n\n(&lt;matplotlib.image.AxesImage at 0x7f7dc21e7730&gt;, tensor(2))\n\n\n\n\n\nEach image train_x[i] has pixel values between 0 and 255 as shown below.\n\ntrain_x[0].min(), train_x[0].max()\n\n(tensor(0, dtype=torch.uint8), tensor(255, dtype=torch.uint8))\n\n\nNow, we normalize the data. Normalization is not necessarily required here, but we are adding it anyways because it offers a host of benefits. For starters, it helps the model converge, i.e., find a minima of the loss function, faster.\n\ntrain_x = train_x.view(-1, 28*28).float() / 255\nvalid_x = valid_x.view(-1, 28*28).float() / 255\ntrain_y = train_y.unsqueeze(1)\nvalid_y = valid_y.unsqueeze(1)\ntrain_x.shape, valid_x.shape, train_y.shape, valid_y.shape\n\n(torch.Size([48000, 784]),\n torch.Size([12000, 784]),\n torch.Size([48000, 1]),\n torch.Size([12000, 1]))\n\n\nNotice how we are unpacking the target variables along the 2nd dimension or as a column vector, so to speak. You’ll learn why as you read through further.\nLet’s have a look at an example from our normalized data.\n\ntrain_x[0].min(), train_x[0].max()\n\n(tensor(0.), tensor(1.))\n\n\nThe pixel values are now between 0 and 1!\nLook at how preparing and preprocessing the data is as crucial as building a model for it!"
  },
  {
    "objectID": "posts/mnist/index.html#loading-the-dataset-for-training-using-dataloader",
    "href": "posts/mnist/index.html#loading-the-dataset-for-training-using-dataloader",
    "title": "Classifying handwritten digits (THE MNIST!)",
    "section": "3. Loading the Dataset for Training using DataLoader",
    "text": "3. Loading the Dataset for Training using DataLoader\nBefore we load the data into a DataLoader, we first have to prepare the data in the proper format for it. DataLoader, as per the documentation takes map-style and iterable-style datasets. So we supply it with one.\n\ntrain_dset = list(zip(train_x, train_y))\nvalid_dset = list(zip(valid_x, valid_y))\n\ntype(valid_dset), type(valid_dset[0])\n\n(list, tuple)\n\n\n\ntrain_dl = DataLoader(train_dset, batch_size=256, shuffle=True)\nvalid_dl = DataLoader(valid_dset, batch_size=256, shuffle=False)\ndls = DataLoaders(train_dl, valid_dl) # fast.ai wrapper that encapsulates train_dl and valid_dl"
  },
  {
    "objectID": "posts/mnist/index.html#creating-a-loss-function",
    "href": "posts/mnist/index.html#creating-a-loss-function",
    "title": "Classifying handwritten digits (THE MNIST!)",
    "section": "4. Creating a Loss Function",
    "text": "4. Creating a Loss Function\nWe now have to create a loss function that suits our dataset. I spent more time figuring out and getting the loss function to work than on the other parts! I’ll put both my original code and the optimized version (by ChatGPT). And no, I will not go through the code because well, my brain’s already fried from writing it 😂. So I suggest you use ChatGPT to explain the code to you or even better, try to figure it out yourselves! It will test your understanding of how the data is structured, and you’ll also have to look at what the model spits out.\n\n# def mse_loss(preds, targets):\n#     preds = preds.sigmoid()\n#     loss = []\n    \n#     for pred, target in zip(preds, targets):\n#         for p in range(len(pred)):\n#             if p == target:\n#                 loss.insert(p, (1 - pred[p])**2)\n#             else:\n#                 loss.insert(p, pred[p]**2)\n#             loss[p] = loss[p].mean().view(1)\n\n#     loss = torch.cat(loss)\n#     return loss.mean()\n\n# Optimized code by ChatGPT\ndef mse_loss(preds, targets):\n    preds = preds.sigmoid()\n    loss = torch.zeros_like(preds)\n\n    for i, target in enumerate(targets):\n        loss[i, target] = (1 - preds[i, target]) ** 2\n        loss[i] += preds[i] ** 2\n\n    return loss.mean()\n\n\n\n\n\n\n\nNote\n\n\n\nI’m using a custom loss function just to demonstrate the performance difference between this loss function (mean squared error) and cross entropy loss (provided by fastai)."
  },
  {
    "objectID": "posts/mnist/index.html#creating-a-neural-network-and-training-it",
    "href": "posts/mnist/index.html#creating-a-neural-network-and-training-it",
    "title": "Classifying handwritten digits (THE MNIST!)",
    "section": "5. Creating a Neural Network and Training it",
    "text": "5. Creating a Neural Network and Training it\nI have chosen the number of neurons and in each layer without any particular reason. So, you can play around with that and see if a lower number works for you as that would bring down the training time. The last layer has to have 10 neurons with output for each digit and that can’t be changed.\nHere, nn.ReLU is used to add non-linearity to the model. Otherwise, the model would still be a linear model no matter how many nn.Linear you add.\n\nn_net = nn.Sequential(\n    nn.Linear(28*28, 250),\n    nn.ReLU(),\n    nn.Linear(250, 50),\n    nn.ReLU(),\n    nn.Linear(50, 10)\n)\n\n’tis time Ladies and Gentlemen! ’tis time to train the model! and fret about how slow MSE is or at least how slow my implementation is\nfastai provides a Learner class that groups together a model, a loss function and a DataLoader object to handle the training. If you want to have a look at the implementation of the entire training process (with minimal to no usage of high-level library functions), then refer to this implementation on a sample 2 digits MNIST dataset.\n\nlearn_mse = Learner(dls, n_net, loss_func=mse_loss, metrics=accuracy)\nlearn_mse.fit(8)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.071596\n0.063623\n0.857333\n01:13\n\n\n1\n0.058387\n0.057266\n0.916000\n01:19\n\n\n2\n0.055814\n0.055529\n0.931000\n01:16\n\n\n3\n0.054634\n0.054546\n0.940333\n01:16\n\n\n4\n0.053810\n0.053983\n0.948583\n01:16\n\n\n5\n0.053269\n0.053475\n0.954667\n01:14\n\n\n6\n0.052900\n0.053110\n0.957667\n01:11\n\n\n7\n0.052528\n0.052865\n0.961500\n01:12\n\n\n\n\n\n\nlearn_ce = Learner(dls, n_net, loss_func=CrossEntropyLossFlat(), metrics=accuracy)\nlearn_ce.fit(8)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.328054\n0.274868\n0.922333\n00:53\n\n\n1\n0.207560\n0.196696\n0.945000\n00:51\n\n\n2\n0.148225\n0.152403\n0.956667\n00:51\n\n\n3\n0.113479\n0.128592\n0.962333\n00:53\n\n\n4\n0.088385\n0.113987\n0.967000\n00:55\n\n\n5\n0.073747\n0.105821\n0.968583\n00:53\n\n\n6\n0.061614\n0.099214\n0.971083\n00:51\n\n\n7\n0.048661\n0.093742\n0.971500\n00:51\n\n\n\n\n\nfastai’s CrossEntropyLossFlat() is clearly faster than our custom loss function and also seems to be giving slightly higher accuracy.\nNow, let’s use our model to make predictions on different digits.\n\nlen(first(train_dl)[0][1]), len(first(train_dl)[1][1])\n\n(784, 1)\n\n\n\ntest_item = first(train_dl) # Gets a batch from the training dataloader\ntorch.softmax(n_net(test_item[0][3]), 0), test_item[1][3] # Note: Do not directly substitute first(dl)[][] in place of test_item.\n# Because first() gives different batches each time it is run\n\n(tensor([1.6710e-05, 3.6341e-01, 5.4452e-01, 1.9914e-02, 8.2348e-05, 8.3202e-05,\n         3.6827e-05, 1.6098e-02, 5.5500e-02, 3.3604e-04],\n        grad_fn=&lt;SoftmaxBackward0&gt;),\n tensor([2]))\n\n\nRun the above code cell as many number of times you want because it gives a different digit as input each time. Well! Looks like we have accomplished what we have set out to do, i.e., classify handwritten digits!\nIf you have followed this as a guide, then congrats on reading through everything! To challenge yourself further you can join one of these Kaggle competitions and make a submission. But note that your “data pipeline” (yes, I feel like a God using this word) for the competition has to be different than what we have done here because you’ll be given a CSV file to work with.\nHere’s a wonderful post that goes into the bits and pieces of how to implement what we have done here from scratch.\nThank you for reading my blog. You can reach out to me through my socials here:\n\nDiscord - “lostsquid.”\nLinkedIn - /in/suchitg04/\n\nI hope to see you soon. Until then 👋"
  },
  {
    "objectID": "posts/data-cleaning/index.html",
    "href": "posts/data-cleaning/index.html",
    "title": "Data Cleaning and Augmentation",
    "section": "",
    "text": "It is a pretty intuitive thought that data is cleaned before training a model so that the model achieves good accuracy. What’s data cleaning in the first place? Naively, it is removing unrelated data that might have creeped through or removing data unrelated to what the model needs in training. I believe you get my point.\nLet’s take an example of a model that you are building to, say, classify between the faces of Elon Musk and Mark Zuckerberg. You’ll probably download the images for your dataset from Google or Bing or any such search engines. With Mark rumored to be an alien and people taking a huge liking to the “female version” of Elon, it is very certain that your dataset will contain a few such memes.\n\n\nAlien Zuckerberg\n\n\n\nElona Musk\n\nNow, sifting through the data in the hopes of finding such memes can be tedious. How about we let the model decide what images are faulty? Here’s how it works. You quickly train a model, the “losses” and accuracies get recorded, and then you pop up some images in decreasing order of “loss” and/or accuracy (there are some tools already that can do that). There you go, the model now helped you find some black sheeps in your data that it found difficult to classify and/or has low confidence about."
  },
  {
    "objectID": "posts/data-cleaning/index.html#data-cleaning",
    "href": "posts/data-cleaning/index.html#data-cleaning",
    "title": "Data Cleaning and Augmentation",
    "section": "",
    "text": "It is a pretty intuitive thought that data is cleaned before training a model so that the model achieves good accuracy. What’s data cleaning in the first place? Naively, it is removing unrelated data that might have creeped through or removing data unrelated to what the model needs in training. I believe you get my point.\nLet’s take an example of a model that you are building to, say, classify between the faces of Elon Musk and Mark Zuckerberg. You’ll probably download the images for your dataset from Google or Bing or any such search engines. With Mark rumored to be an alien and people taking a huge liking to the “female version” of Elon, it is very certain that your dataset will contain a few such memes.\n\n\nAlien Zuckerberg\n\n\n\nElona Musk\n\nNow, sifting through the data in the hopes of finding such memes can be tedious. How about we let the model decide what images are faulty? Here’s how it works. You quickly train a model, the “losses” and accuracies get recorded, and then you pop up some images in decreasing order of “loss” and/or accuracy (there are some tools already that can do that). There you go, the model now helped you find some black sheeps in your data that it found difficult to classify and/or has low confidence about."
  },
  {
    "objectID": "posts/data-cleaning/index.html#data-augmentation",
    "href": "posts/data-cleaning/index.html#data-augmentation",
    "title": "Data Cleaning and Augmentation",
    "section": "Data Augmentation",
    "text": "Data Augmentation\nThe world’s gone dystopian and governments are crumbling, and Elon and Mark have decided to collaborate and take advantage of this calamity. You are the hero in this situation.\nYou observe that there is a lot of movement in and out of the abandoned Twitter office (Elon’s selling tickets to Mars to the elites lol). Looks like someone’s having some in-person meetings 🤨. Now you install a camera along with a image/video recognition model to alert you whenever Elon or Mark comes and goes in/out of the office, but you don’t get even a single trigger for days!\nYou scratch your head thinking about what could be wrong for hours until you realise that you had trained the model using just headshots and few such “presentable” images that you scavenged from what’s left of the internet, but you have installed your camera in such a place that it does not get such good images.\nWhat’s the solution you may ask? This is where Data Augmentation comes into the picture. You use this technique and apply certain effects on the images like cropping of random parts of the image, applying different colour filters, distorting the image, etc. Not only does this expand the dataset, but it also enables the model to better understand the object it is learning (Elon and Mark, in this case).\nHere’s an example of what data augmentation does: \nData augmentation is particularly useful when you have a small dataset. It helps bring some variance and helps avoid overfitting if done properly. Give this interesting article a read: Regularization Effect of Data Augmentation.\nCover photo by Elīna Arāja.\nThank you for reading my blog. You can reach out to me through my socials here:\n\nDiscord - “lostsquid.”\nLinkedIn - /in/suchitg04/\n\nI hope to see you soon. Until then 👋"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AsquirousSpeaks",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n\n\n\n\n  \n\n\n\n\nClassifying handwritten digits (THE MNIST!)\n\n\n\n\n\n\n\nNeural Networks\n\n\nfast.ai\n\n\nComputerVision\n\n\n\n\nUnderstand the fundamental structure of a ML project by building a simple neural network on the MNIST dataset!\n\n\n\n\n\n\nSep 9, 2023\n\n\nSuchit G\n\n\n\n\n\n\n  \n\n\n\n\nData Cleaning and Augmentation\n\n\n\n\n\n\n\nData Processing\n\n\nBeginner\n\n\n\n\nThe counter-intuitive idea of data cleaning after training a model and creating data by ourselves.\n\n\n\n\n\n\nJun 27, 2023\n\n\nSuchit G\n\n\n\n\n\n\n  \n\n\n\n\nTrain your first image classifier (AI) model!\n\n\n\n\n\n\n\nBeginner\n\n\nComputerVision\n\n\nfast.ai\n\n\n\n\nGet a taste of AI/deep learning with this simple tutorial that trains an image classifier model!\n\n\n\n\n\n\nMay 29, 2023\n\n\nSuchit G\n\n\n\n\n\n\n  \n\n\n\n\nHey yo! Welcome to my blog peeps!\n\n\n\n\n\n\n\nExperience\n\n\n\n\nHiya! Know more about my blog!\n\n\n\n\n\n\nMay 15, 2023\n\n\nSuchit G\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Hey yo! Welcome to my blog peeps!",
    "section": "",
    "text": "Hey there! My name is Suchit. I am a freshman studying Information Science (or CS, IS is just a marketing term here XD) at RIT, Bangalore.\nIn this blog I hope to share my learnings and mostly tech stuff. My aim is to help the Suchit from a week or a month or an year ago, so that people going through the same problem/thoughts can gain better clarity. The added bonus is that I’ll still be able to remember how I felt during that situation, therefore enabling me to provide you with help that’ll actually help you.\nWith that said, you can expect my blogs to be short and have hints of GenZ humour here and there.\nFeel free to reach out to me through my socials:\n\nDiscord - “lostsquid.”\n\nLinkedIn - /in/suchitg04/\n\nI hope to see you soon! Until then 👋"
  },
  {
    "objectID": "posts/tvdesktopclassifier/index.html",
    "href": "posts/tvdesktopclassifier/index.html",
    "title": "Train your first image classifier (AI) model!",
    "section": "",
    "text": "Want to get hands-on experience with AI? If so, this is the perfect tutorial for you! Pre-requisites for this tutorial:\n\nbeginner-level python\nbe able to use jupyter notebooks\na Kaggle account (go create one duh)\n\nThat’s all! If you don’t know how to use Jupyter notebooks, click here for a quick tutorial on both Jupyter notebooks and Kaggle.\nSo what exactly will we be creating in this tutorial? We are gonna train a Deep Learning model to identify if a given image is a CRT TV or a flat screen TV or a desktop monitor. We will be using a beginner friendly and widely used library called fastai. Sounds damn cool, at least to me :)\nNote 1: You need not break your head over what each line of code does. This tutorial is meant to give you impetus to delve into Deep Learning and a top level overview of it’s power :)\nNote 2: GPU needs to be enabled for this tutorial or you’ll be spending hours training the model 😂. Specifically enable GPU P100. Also, to use GPUs you need to have your phone number verified, so go do that if you haven’t yet."
  },
  {
    "objectID": "posts/tvdesktopclassifier/index.html#introduction",
    "href": "posts/tvdesktopclassifier/index.html#introduction",
    "title": "Train your first image classifier (AI) model!",
    "section": "",
    "text": "Want to get hands-on experience with AI? If so, this is the perfect tutorial for you! Pre-requisites for this tutorial:\n\nbeginner-level python\nbe able to use jupyter notebooks\na Kaggle account (go create one duh)\n\nThat’s all! If you don’t know how to use Jupyter notebooks, click here for a quick tutorial on both Jupyter notebooks and Kaggle.\nSo what exactly will we be creating in this tutorial? We are gonna train a Deep Learning model to identify if a given image is a CRT TV or a flat screen TV or a desktop monitor. We will be using a beginner friendly and widely used library called fastai. Sounds damn cool, at least to me :)\nNote 1: You need not break your head over what each line of code does. This tutorial is meant to give you impetus to delve into Deep Learning and a top level overview of it’s power :)\nNote 2: GPU needs to be enabled for this tutorial or you’ll be spending hours training the model 😂. Specifically enable GPU P100. Also, to use GPUs you need to have your phone number verified, so go do that if you haven’t yet."
  },
  {
    "objectID": "posts/tvdesktopclassifier/index.html#installing-modules",
    "href": "posts/tvdesktopclassifier/index.html#installing-modules",
    "title": "Train your first image classifier (AI) model!",
    "section": "2. Installing modules",
    "text": "2. Installing modules\nHere, we will be installing/updating the python modules necessary for this tutorial.\nRun the two following code blocks to do so.\n\nimport os\nfrom fastcore.all import *\nimport urllib.request\nfrom fastai.vision.all import *\nfrom fastdownload import download_url\n\niskaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')\n\nif iskaggle:\n    !pip install -Uqq fastai duckduckgo_search\n\nfrom duckduckgo_search import ddg_images\n\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npandas-profiling 3.6.2 requires requests&lt;2.29,&gt;=2.24.0, but you have requests 2.31.0 which is incompatible.\nlibrosa 0.10.0.post2 requires soundfile&gt;=0.12.1, but you have soundfile 0.11.0 which is incompatible.\napache-beam 2.44.0 requires dill&lt;0.3.2,&gt;=0.3.1.1, but you have dill 0.3.6 which is incompatible."
  },
  {
    "objectID": "posts/tvdesktopclassifier/index.html#glimpsing-our-data",
    "href": "posts/tvdesktopclassifier/index.html#glimpsing-our-data",
    "title": "Train your first image classifier (AI) model!",
    "section": "3. Glimpsing our data",
    "text": "3. Glimpsing our data\nWhat is AI without data? Data is arguably the most important thing in the field of AI/Data Science. AI models are trained using tons and tons of data. Let’s have a look at how our data looks like.\nIn the first block, we search and retrieve one URL for an image of a flat screen TV. We then download the image from the retrieved URL and open the image and repeat the process for a CRT TV in the subsequent code blocks.\n\ndef search_images(term, max_images=40):\n    print(f\"Searching for '{term}'\")\n    return L(ddg_images(term, max_results=max_images)).itemgot('image')\n\n\nurls = search_images('flat screen tv', max_images=1)\nurls[0]\n\nSearching for 'flat screen tv'\n\n\n/opt/conda/lib/python3.7/site-packages/duckduckgo_search/compat.py:60: UserWarning: ddg_images is deprecated. Use DDGS().images() generator\n  warnings.warn(\"ddg_images is deprecated. Use DDGS().images() generator\")\n/opt/conda/lib/python3.7/site-packages/duckduckgo_search/compat.py:64: UserWarning: parameter page is deprecated\n  warnings.warn(\"parameter page is deprecated\")\n/opt/conda/lib/python3.7/site-packages/duckduckgo_search/compat.py:66: UserWarning: parameter max_results is deprecated\n  warnings.warn(\"parameter max_results is deprecated\")\n\n\n'http://s4msungtelevision32.files.wordpress.com/2013/01/flat-screen-televisions.jpg'\n\n\n\ndest = 'flatscreentv.jpg'\n# Trying two libraries because they both are working erratically for me\n# try:\n#     download_url(urls[0], dest, show_progress=False)\n#     print(\"hi\")\n# except:\nurllib.request.urlretrieve(urls[0], dest)\n\n('flatscreentv.jpg', &lt;http.client.HTTPMessage at 0x7e8f7900d090&gt;)\n\n\n\nImage.open(dest).to_thumb(256, 256)\n\n\n\n\n\ntry:\n    download_url(search_images('crt tv', max_images=1)[0], 'crttv.jpg', show_progress=False)\nexcept:\n    urllib.request.urlretrieve(search_images('crt tv', max_images=1)[0], 'crttv.jpg')\n\nImage.open('crttv.jpg').to_thumb(256, 256)\n\nSearching for 'crt tv'"
  },
  {
    "objectID": "posts/tvdesktopclassifier/index.html#downloading-images",
    "href": "posts/tvdesktopclassifier/index.html#downloading-images",
    "title": "Train your first image classifier (AI) model!",
    "section": "4. Downloading images",
    "text": "4. Downloading images\nLet us now download a bunch of images to train our model and make them ready to be “fed” into the model.\n\n# Downloading images into their respective directories\nsearches = 'flat screen tv', 'crt tv', 'desktop monitor'\npath = Path('tv_or_desktop')\nfrom time import sleep\n\nfor o in searches:\n    dest = (path/o)\n    dest.mkdir(exist_ok=True, parents=True)\n    download_images(dest, urls=search_images(o))\n    sleep(10)\n    resize_images(path/o, max_size=400, dest=path/o)\n\nSearching for 'flat screen tv'\nSearching for 'crt tv'\nSearching for 'desktop monitor'\n\n\n\n# Check and remove unopenable images\nfailed = verify_images(get_image_files(path))\nfailed.map(Path.unlink)\nlen(failed)\n\n7\n\n\n\n# Loading the data\ndls = DataBlock(\n    blocks=(ImageBlock, CategoryBlock),\n    get_items=get_image_files,\n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=parent_label,\n    item_tfms=[Resize(192, method='squish')]\n).dataloaders(path, bs=32)\n\ndls.show_batch(max_n=9)"
  },
  {
    "objectID": "posts/tvdesktopclassifier/index.html#training-our-model",
    "href": "posts/tvdesktopclassifier/index.html#training-our-model",
    "title": "Train your first image classifier (AI) model!",
    "section": "5. Training our model!",
    "text": "5. Training our model!\nThis is it people. ’tis time to train our model! For this example we use a model called ResNet18 with 18 layers. ResNet18 is pre-trained on ImageNet dataset. Therefore, we need not train it again, rather we fine tune it to recognize images from our dataset, i.e., flatscreen TV, CRT TV and desktop monitors.\n\nlearn = vision_learner(dls, resnet18, metrics=error_rate)\nlearn.fine_tune(5)\n\n/opt/conda/lib/python3.7/site-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and may be removed in the future, \"\n/opt/conda/lib/python3.7/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.542415\n1.480012\n0.523810\n00:06\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.913363\n1.170745\n0.476190\n00:01\n\n\n1\n0.876068\n0.878507\n0.333333\n00:01\n\n\n2\n0.637958\n0.758396\n0.238095\n00:01\n\n\n3\n0.499402\n0.701759\n0.190476\n00:01\n\n\n4\n0.409540\n0.639917\n0.190476\n00:01"
  },
  {
    "objectID": "posts/tvdesktopclassifier/index.html#classifying-images",
    "href": "posts/tvdesktopclassifier/index.html#classifying-images",
    "title": "Train your first image classifier (AI) model!",
    "section": "6. Classifying images",
    "text": "6. Classifying images\nCongratulations! You have trained your first (ig 🤷) image classification model! Let us now put it to test and try classifying some images.\n\n# Predicting an image we had downloaded earlier\nlearn.predict(PILImage.create('crttv.jpg'))\n\n\n\n\n\n\n\n\n('crt tv', tensor(0), tensor([9.9726e-01, 5.5894e-04, 2.1807e-03]))\n\n\n\n# Predicting an image we had downloaded earlier\nlearn.predict(PILImage.create('flatscreentv.jpg'))\n\n\n\n\n\n\n\n\n('desktop monitor', tensor(1), tensor([6.4813e-04, 9.9257e-01, 6.7823e-03]))\n\n\n\n# download_url(search_images('desktop monitor', max_images=1)[0], dest='desktopmonitor.jpg', show_progress=False)\ntry:\n    download_url(search_images('desktop monitor', max_images=1)[0], 'desktopmonitor.jpg', show_progress=False)\nexcept:\n    urllib.request.urlretrieve(search_images('desktop monitor', max_images=1)[0], 'desktopmonitor.jpg')\n\nImage.open('desktopmonitor.jpg').to_thumb(256, 256)\n\nSearching for 'desktop monitor'\n\n\n\n\n\n\nlearn.predict(PILImage.create('desktopmonitor.jpg'))\n\n\n\n\n\n\n\n\n('desktop monitor', tensor(1), tensor([5.4398e-05, 9.9916e-01, 7.8518e-04]))\n\n\nWell, that’s not bad for our first model. It has a decent accuracy. For me, it got 2/3 predictions right.\nIf you have any questions don’t hesitate to message me on discord. And lastly, here’s the link to my notebook if you wannaplay around with it (click on the “copy and edit button”).\nThank you for reading my blog. You can reach out to me through my socials here:\n\nDiscord - “lostsquid.”\nLinkedIn - /in/suchitg04/\n\nI hope to see you soon. Until then 👋"
  }
]